{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ML Python Code Generation using Causal Language Model\n\n**In this Notebook, we aim to construct a Causal Language Model designed specifically for Python code generation. The main focus is to create a model capable of taking a fragment of Python code as input and generating the entire code sequence as output. The significance of this approach lies in its potential to streamline the coding process and enhance efficiency.**\n\n**Due to the vast landscape of Python libraries and frameworks, we'll narrow down the scope of our model to cater specifically to Data Science libraries. This decision is driven by the desire to optimize resources and reduce processing time. Therefore, our model will be tailored to four essential Data Science libraries:**\n\n- **Pandas**: A powerful library for data manipulation and analysis, providing versatile data structures and tools.\n\n- **Matplotlib**: A widely-used library for creating static, interactive, and animated visualizations in Python.\n\n- **Seaborn**: Built on top of Matplotlib, this library facilitates the creation of attractive statistical graphics.\n\n- **Scikit-Learn**: An invaluable library for machine learning tasks, offering a wide array of algorithms and tools.\n\n**By focusing on these libraries, we can harness the capabilities of the Causal Language Model to deliver efficient and accurate Python code generation, specifically tailored to Data Science tasks. Let's dive into the implementation and explore the potential of this novel approach!\"**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-26T15:53:08.977362Z","iopub.execute_input":"2023-07-26T15:53:08.978188Z","iopub.status.idle":"2023-07-26T15:53:08.998450Z","shell.execute_reply.started":"2023-07-26T15:53:08.978158Z","shell.execute_reply":"2023-07-26T15:53:08.994285Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#Install the necessary libraries\n!pip install -q transformers\n!pip install -q datasets","metadata":{"execution":{"iopub.status.busy":"2023-07-26T15:53:09.023633Z","iopub.execute_input":"2023-07-26T15:53:09.024675Z","iopub.status.idle":"2023-07-26T15:53:38.054894Z","shell.execute_reply.started":"2023-07-26T15:53:09.024642Z","shell.execute_reply":"2023-07-26T15:53:38.053505Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Set the seed value\nSEED = 4243","metadata":{"execution":{"iopub.status.busy":"2023-07-26T15:53:38.058204Z","iopub.execute_input":"2023-07-26T15:53:38.058592Z","iopub.status.idle":"2023-07-26T15:53:38.065427Z","shell.execute_reply.started":"2023-07-26T15:53:38.058552Z","shell.execute_reply":"2023-07-26T15:53:38.064371Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Dataset\n\n**Regarding the dataset used for training the Causal Language Model, the ideal candidate would be the [Codeparrot dataset](https://huggingface.co/datasets/transformersbook/codeparrot). However, due to its substantial size of nearly 180GB and its demanding computational requirements, we will opt for a more manageable version of this dataset. A [smaller](https://huggingface.co/datasets/huggingface-course/codeparrot-ds-train), curated version is conveniently accessible, and it will sufficiently serve our purposes.**\n\n**This curated dataset is designed to provide a representative sample of Python code from GitHub repositories. While it may not encompass the entirety of the original dataset, it still contains diverse and relevant code examples. This scaled-down version helps conserve computational resources and facilitates a smoother and faster training process.**\n\n**By utilizing this reduced dataset, we can still achieve substantial learning outcomes and generate Python code effectively. So, let's proceed with this more manageable dataset and explore the potentials of our Causal Language Model in Python code generation for Data Science libraries!**","metadata":{}},{"cell_type":"code","source":"#Download the dataset\nfrom datasets import load_dataset, DatasetDict\n\ntrain = load_dataset(path=\"huggingface-course/codeparrot-ds-train\", split=\"train\")\nvalid = load_dataset(path=\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n\n#Combine the train and valid dataset into a single DatasetDict object\n\ndataset = DatasetDict(\n        {\n        \"train\": train,\n        \"valid\": valid\n        }\n                     )","metadata":{"execution":{"iopub.status.busy":"2023-07-26T15:53:38.066805Z","iopub.execute_input":"2023-07-26T15:53:38.067283Z","iopub.status.idle":"2023-07-26T15:59:19.062247Z","shell.execute_reply.started":"2023-07-26T15:53:38.067196Z","shell.execute_reply":"2023-07-26T15:59:19.061239Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/huggingface-course--codeparrot-ds-train to /root/.cache/huggingface/datasets/json/huggingface-course--codeparrot-ds-train-a9b1bc4c2b855d04/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da96d3c9a10841939947d4b893e07c0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/8.25G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d0253f185584b45ac20aea0613621e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fb4a90cf45f4c68af6fc49f444f6a92"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/huggingface-course--codeparrot-ds-train-a9b1bc4c2b855d04/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\nDownloading and preparing dataset json/huggingface-course--codeparrot-ds-valid to /root/.cache/huggingface/datasets/json/huggingface-course--codeparrot-ds-valid-e5ece22bd7b6a6ac/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e459ca98004127ae5a83c3ac231c7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/46.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a8a27cad3a24e6ba5e8f62aa9db1c10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab98bc9e294245e39e7fb1ade384c8cf"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/huggingface-course--codeparrot-ds-valid-e5ece22bd7b6a6ac/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"code","source":"#Have a look at the dataset fields\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-07-26T15:59:19.064920Z","iopub.execute_input":"2023-07-26T15:59:19.065894Z","iopub.status.idle":"2023-07-26T15:59:19.075588Z","shell.execute_reply.started":"2023-07-26T15:59:19.065854Z","shell.execute_reply":"2023-07-26T15:59:19.074646Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n        num_rows: 606720\n    })\n    valid: Dataset({\n        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n        num_rows: 3322\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"#Interview the data / Have a look at some samples\nfor key, value in dataset[\"train\"][0].items():\n    print(f\">>>> {key}: {value[:1_000]}\\n\")\n#Since the content field contains long codes, we have restrcit it to 1,000 characters","metadata":{"execution":{"iopub.status.busy":"2023-07-26T15:59:19.076776Z","iopub.execute_input":"2023-07-26T15:59:19.077774Z","iopub.status.idle":"2023-07-26T15:59:19.191514Z","shell.execute_reply.started":"2023-07-26T15:59:19.077741Z","shell.execute_reply":"2023-07-26T15:59:19.190539Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":">>>> repo_name: kmike/scikit-learn\n\n>>>> path: sklearn/utils/__init__.py\n\n>>>> copies: 3\n\n>>>> size: 10094\n\n>>>> content: \"\"\"\nThe :mod:`sklearn.utils` module includes various utilites.\n\"\"\"\n\nfrom collections import Sequence\n\nimport numpy as np\nfrom scipy.sparse import issparse\nimport warnings\n\nfrom .murmurhash import murmurhash3_32\nfrom .validation import (as_float_array, check_arrays, safe_asarray,\n                         assert_all_finite, array2d, atleast2d_or_csc,\n                         atleast2d_or_csr, warn_if_not_float,\n                         check_random_state)\nfrom .class_weight import compute_class_weight\n\n__all__ = [\"murmurhash3_32\", \"as_float_array\", \"check_arrays\", \"safe_asarray\",\n           \"assert_all_finite\", \"array2d\", \"atleast2d_or_csc\",\n           \"atleast2d_or_csr\", \"warn_if_not_float\", \"check_random_state\",\n           \"compute_class_weight\"]\n\n# Make sure that DeprecationWarning get printed\nwarnings.simplefilter(\"always\", DeprecationWarning)\n\n\nclass deprecated(object):\n    \"\"\"Decorator to mark a function or class as deprecated.\n\n    Issue a warning when the function is called/the c\n\n>>>> license: bsd-3-clause\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Preprocessing\n\n**In the data preprocessing phase, we need to perform two essential steps to prepare the text data for the Causal Language Model: tokenization and data collation.**\n\n#### Tokenization:\nIt involves breaking down the raw text (Python code in this case) into smaller units called tokens. These tokens could be individual words, subwords, or characters, depending on the specific requirements of the model. Once tokenized, the next step is to convert these tokens into numerical representations. This conversion enables the model to understand and process the data effectively, as neural networks work with numerical inputs.\n\n#### Data Collation:\nAfter tokenization, the data needs to be organized into sequences that the Causal Language Model can use for training. Since models typically learn from fixed-length sequences, we create data samples by sliding a window over the tokenized text. This means we extract consecutive token sequences from the code, making each sequence a training instance. By using this approach, the model can learn patterns and dependencies within the code, which is crucial for accurate Python code generation.\n\n**Together, these two preprocessing steps pave the way for successful training of the Causal Language Model, enabling it to effectively generate Python code based on the patterns it learns from the prepared tokenized and collated data.**","metadata":{}},{"cell_type":"code","source":"#Instantiate the tokenizer\nfrom transformers import AutoTokenizer\n\ncontext_length = 128\ntokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2023-07-26T15:59:19.192748Z","iopub.execute_input":"2023-07-26T15:59:19.193045Z","iopub.status.idle":"2023-07-26T15:59:23.678487Z","shell.execute_reply.started":"2023-07-26T15:59:19.192995Z","shell.execute_reply":"2023-07-26T15:59:23.677328Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/265 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f50562d1cc374b22b66d92b945f60263"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/789k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6042772f6d674a93a8da75e9325aa94d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/448k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a046a3f282994a84a2ff7993e365ff96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15d218429dba4028b6774c5cea4c3495"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b79684b548c4933bab1e1642370f7a8"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Before tokenizing the whole dataset, it is a good practice to check it on the sample inputs.**","metadata":{}},{"cell_type":"code","source":"#Let's tokenzie first 2 samples\nsamp_token = tokenizer(\n    dataset[\"train\"][:2][\"content\"],\n    truncation=True,\n    max_length=context_length,\n    return_overflowing_tokens=True,\n    return_length=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-26T15:59:23.680145Z","iopub.execute_input":"2023-07-26T15:59:23.681259Z","iopub.status.idle":"2023-07-26T15:59:23.720367Z","shell.execute_reply.started":"2023-07-26T15:59:23.681216Z","shell.execute_reply":"2023-07-26T15:59:23.719455Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#Check the fields in the tokenized object\nsamp_token.keys()","metadata":{"execution":{"iopub.status.busy":"2023-07-26T15:59:23.721790Z","iopub.execute_input":"2023-07-26T15:59:23.722153Z","iopub.status.idle":"2023-07-26T15:59:23.730921Z","shell.execute_reply.started":"2023-07-26T15:59:23.722118Z","shell.execute_reply":"2023-07-26T15:59:23.730136Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"dict_keys(['input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'])"},"metadata":{}}]},{"cell_type":"code","source":"print(f\"Input IDs length: {len(samp_token['input_ids'])}\")\nprint(f\"Input chunk lengths: {(samp_token['length'])}\")\nprint(f\"Chunk mapping: {samp_token['overflow_to_sample_mapping']}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-26T15:59:23.732337Z","iopub.execute_input":"2023-07-26T15:59:23.733069Z","iopub.status.idle":"2023-07-26T15:59:23.739806Z","shell.execute_reply.started":"2023-07-26T15:59:23.733033Z","shell.execute_reply":"2023-07-26T15:59:23.738853Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Input IDs length: 34\nInput chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\nChunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Since the tokenizer works fine on the sample data, we can apply it on the whole dataset using Dataset.map() method**","metadata":{}},{"cell_type":"code","source":"#Define a function to tokenzie the whole dataset\ndef tokenize_ftn(element):\n    outputs = tokenizer(\n        element[\"content\"],\n        truncation=True,\n        max_length=context_length,\n        return_overflowing_tokens=True,\n        return_length=True,\n    )\n    input_batch = []\n    \n    #Discard the tokens that are smaller than the context size\n    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n        if length == context_length:\n            input_batch.append(input_ids)\n    return {\"input_ids\": input_batch}","metadata":{"execution":{"iopub.status.busy":"2023-07-26T15:59:23.744518Z","iopub.execute_input":"2023-07-26T15:59:23.745487Z","iopub.status.idle":"2023-07-26T15:59:23.751577Z","shell.execute_reply.started":"2023-07-26T15:59:23.745453Z","shell.execute_reply":"2023-07-26T15:59:23.750936Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#Check the tokenization function on some sample input\nfor sample in dataset[\"train\"]:\n    tokenized_sample = tokenize_ftn(sample)\n    break\ntokenized_sample.keys()","metadata":{"execution":{"iopub.status.busy":"2023-07-26T15:59:23.752799Z","iopub.execute_input":"2023-07-26T15:59:23.753769Z","iopub.status.idle":"2023-07-26T15:59:23.773301Z","shell.execute_reply.started":"2023-07-26T15:59:23.753733Z","shell.execute_reply":"2023-07-26T15:59:23.772603Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"dict_keys(['input_ids'])"},"metadata":{}}]},{"cell_type":"code","source":"#Apply the tokenization function on the whole dataset\ntokenized_dataset = dataset.map(function=tokenize_ftn,\n                                batched=True,\n                                remove_columns=dataset[\"train\"].column_names)\ntokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-26T15:59:23.774451Z","iopub.execute_input":"2023-07-26T15:59:23.775401Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/607 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6b83beed8d644f2a3ac0487d3dbd248"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Model\n\n**In our pursuit of generating Python code with utmost proficiency, we will harness the capabilities of the GPT-2 language model. Having undergone extensive training on an extensive corpus of text, GPT-2 has developed a remarkable aptitude for comprehending and predicting patterns within the provided data. To tailor the model to our specific needs, we can fine-tune a pre-trained GPT-2 model.**\n\n**Nonetheless, given the substantial amount of data available, a prudent approach would be to train the model from scratch. This allows us to customize the model's understanding of Python code and adapt it precisely to the intricacies of our dataset. Training from scratch ensures that the model becomes intimately familiar with the specific nuances and complexities of Python syntax, thereby optimizing its code generation capabilities.**","metadata":{}},{"cell_type":"code","source":"#Load the configuration file for the GPT-2 model\nfrom transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig\n\n#Load the configuration file for the GPT-2 model\nconfig = AutoConfig.from_pretrained(pretrained_model_name_or_path=\"gpt2\",\n                                    vocab_size=len(tokenizer),\n                                    n_ctx=context_length,\n                                    bos_token_id=tokenizer.bos_token_id,\n                                    eos_token_id=tokenizer.eos_token_id,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print the configuration file\nconfig","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Instantiate the model with the configuration file\nmodel = TFGPT2LMHeadModel(config=config)\nmodel(model.dummy_inputs)  # Builds the model\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Instantiate the data collator\nfrom transformers import DataCollatorForLanguageModeling\n\ntokenizer.pad_token = tokenizer.eos_token\ndata_collator = DataCollatorForLanguageModeling(tokenizer,\n                                                mlm=False,\n                                                return_tensors=\"tf\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check the data collator on some samples\ncollated_sample = data_collator([tokenized_dataset[\"train\"][i] for i in range(5)])\nfor key in collated_sample:\n    print(f\"{key} shape: {collated_sample[key].shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create tf.data.Dataset object\ntf_train_dataset = model.prepare_tf_dataset(tokenized_dataset[\"train\"],\n                                            collate_fn=data_collator,\n                                            shuffle=True,\n                                            batch_size=32)\n\ntf_eval_dataset = model.prepare_tf_dataset(tokenized_dataset[\"valid\"],\n                                           collate_fn=data_collator,\n                                           shuffle=False,\n                                           batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Log in to the HuggingFace account\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define the optimizer\nfrom transformers import create_optimizer\nimport tensorflow as tf\n\nnum_train_steps = len(tf_train_dataset)\noptimizer, schedule = create_optimizer(init_lr=5e-5,\n                                       num_warmup_steps=1_000,\n                                       num_train_steps=num_train_steps,\n                                       weight_decay_rate=0.01)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Complie the model\nmodel.compile(optimizer=optimizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define the callbacks\nfrom transformers.keras_callbacks import PushToHubCallback\n\ncallback = PushToHubCallback(output_dir=\"python-code-generator\",\n                             tokenizer=tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train the model\nmodel.fit(tf_train_dataset,\n          validation_data=tf_eval_dataset,\n          callbacks=[callback])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}