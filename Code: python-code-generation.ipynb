{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ML Python Code Generation using Causal Language Model\n\n**In this Notebook, we aim to construct a Causal Language Model designed specifically for Python code generation. The main focus is to create a model capable of taking a fragment of Python code as input and generating the entire code sequence as output. The significance of this approach lies in its potential to streamline the coding process and enhance efficiency.**\n\n**Due to the vast landscape of Python libraries and frameworks, we'll narrow down the scope of our model to cater specifically to Data Science libraries. This decision is driven by the desire to optimize resources and reduce processing time. Therefore, our model will be tailored to four essential Data Science libraries:**\n\n- **Pandas**: A powerful library for data manipulation and analysis, providing versatile data structures and tools.\n\n- **Matplotlib**: A widely-used library for creating static, interactive, and animated visualizations in Python.\n\n- **Seaborn**: Built on top of Matplotlib, this library facilitates the creation of attractive statistical graphics.\n\n- **Scikit-Learn**: An invaluable library for machine learning tasks, offering a wide array of algorithms and tools.\n\n**By focusing on these libraries, we can harness the capabilities of the Causal Language Model to deliver efficient and accurate Python code generation, specifically tailored to Data Science tasks. Let's dive into the implementation and explore the potential of this novel approach!\"**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-27T09:35:49.175391Z","iopub.execute_input":"2023-07-27T09:35:49.175928Z","iopub.status.idle":"2023-07-27T09:35:49.188538Z","shell.execute_reply.started":"2023-07-27T09:35:49.175892Z","shell.execute_reply":"2023-07-27T09:35:49.187164Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#Install the necessary libraries\n!pip install -q transformers\n!pip install -q datasets","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:35:49.282514Z","iopub.execute_input":"2023-07-27T09:35:49.282777Z","iopub.status.idle":"2023-07-27T09:36:12.852269Z","shell.execute_reply.started":"2023-07-27T09:35:49.282754Z","shell.execute_reply":"2023-07-27T09:36:12.850961Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Set the seed value\nSEED = 4243","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:36:12.854884Z","iopub.execute_input":"2023-07-27T09:36:12.855333Z","iopub.status.idle":"2023-07-27T09:36:12.860192Z","shell.execute_reply.started":"2023-07-27T09:36:12.855292Z","shell.execute_reply":"2023-07-27T09:36:12.859232Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Dataset\n\n**Regarding the dataset used for training the Causal Language Model, the ideal candidate would be the [Codeparrot dataset](https://huggingface.co/datasets/transformersbook/codeparrot). However, due to its substantial size of nearly 180GB and its demanding computational requirements, we will opt for a more manageable version of this dataset. A [smaller](https://huggingface.co/datasets/huggingface-course/codeparrot-ds-train), curated version is conveniently accessible, and it will sufficiently serve our purposes.**\n\n**This curated dataset is designed to provide a representative sample of Python code from GitHub repositories. While it may not encompass the entirety of the original dataset, it still contains diverse and relevant code examples. This scaled-down version helps conserve computational resources and facilitates a smoother and faster training process.**\n\n**By utilizing this reduced dataset, we can still achieve substantial learning outcomes and generate Python code effectively. So, let's proceed with this more manageable dataset and explore the potentials of our Causal Language Model in Python code generation for Data Science libraries!**","metadata":{}},{"cell_type":"code","source":"#Download the dataset\nfrom datasets import load_dataset, DatasetDict\n\ntrain = load_dataset(path=\"huggingface-course/codeparrot-ds-train\", split=\"train\")\nvalid = load_dataset(path=\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:36:12.861678Z","iopub.execute_input":"2023-07-27T09:36:12.862008Z","iopub.status.idle":"2023-07-27T09:43:10.203667Z","shell.execute_reply.started":"2023-07-27T09:36:12.861976Z","shell.execute_reply":"2023-07-27T09:43:10.201416Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/huggingface-course--codeparrot-ds-train to /root/.cache/huggingface/datasets/json/huggingface-course--codeparrot-ds-train-a9b1bc4c2b855d04/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"001f7d49576a4f1ba9360e30950d0f02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/8.25G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bacbe4385cb45d0b38c40009eb3fdef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1cb6963a898486194c81e8c5f99c3db"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/huggingface-course--codeparrot-ds-train-a9b1bc4c2b855d04/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\nDownloading and preparing dataset json/huggingface-course--codeparrot-ds-valid to /root/.cache/huggingface/datasets/json/huggingface-course--codeparrot-ds-valid-e5ece22bd7b6a6ac/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d096450aa1e844de85fa209b3bf01d5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/46.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e672b2a14c14ee9a3ed94ea7672018e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c31ac7cc42c4fc188856890fbd2d969"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/huggingface-course--codeparrot-ds-valid-e5ece22bd7b6a6ac/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"code","source":"#Combine the train and valid dataset into a single DatasetDict object\ndataset = DatasetDict(\n        {\n        \"train\": train.shuffle(seed=SEED).select(range(5_000)),\n        \"valid\": valid.shuffle(seed=SEED).select(range(5_00))\n        }\n                     )\n#Training the whole data would require more computation than available\n# GPU hours. So, we'll restrict the data to 5K samples only","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:43:10.207306Z","iopub.execute_input":"2023-07-27T09:43:10.208022Z","iopub.status.idle":"2023-07-27T09:43:10.581943Z","shell.execute_reply.started":"2023-07-27T09:43:10.207994Z","shell.execute_reply":"2023-07-27T09:43:10.580938Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Have a look at the dataset fields\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:43:10.583336Z","iopub.execute_input":"2023-07-27T09:43:10.584873Z","iopub.status.idle":"2023-07-27T09:43:10.593060Z","shell.execute_reply.started":"2023-07-27T09:43:10.584832Z","shell.execute_reply":"2023-07-27T09:43:10.592105Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n        num_rows: 5000\n    })\n    valid: Dataset({\n        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n        num_rows: 500\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"#Interview the data / Have a look at some samples\nfor key, value in dataset[\"train\"][0].items():\n    print(f\">>>> {key}: {value[:1_000]}\\n\")\n#Since the content field contains long codes, we have restrcit it to 1,000 characters","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:43:10.594386Z","iopub.execute_input":"2023-07-27T09:43:10.595326Z","iopub.status.idle":"2023-07-27T09:43:10.610631Z","shell.execute_reply.started":"2023-07-27T09:43:10.595289Z","shell.execute_reply":"2023-07-27T09:43:10.609569Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":">>>> repo_name: paladin74/neural-network-animation\n\n>>>> path: matplotlib/axis.py\n\n>>>> copies: 10\n\n>>>> size: 80644\n\n>>>> content: \"\"\"\nClasses for the ticks and x and y axis\n\"\"\"\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport six\n\nfrom matplotlib import rcParams\nimport matplotlib.artist as artist\nfrom matplotlib.artist import allow_rasterization\nimport matplotlib.cbook as cbook\nimport matplotlib.font_manager as font_manager\nimport matplotlib.lines as mlines\nimport matplotlib.patches as mpatches\nimport matplotlib.scale as mscale\nimport matplotlib.text as mtext\nimport matplotlib.ticker as mticker\nimport matplotlib.transforms as mtransforms\nimport matplotlib.units as munits\nimport numpy as np\nimport warnings\n\nGRIDLINE_INTERPOLATION_STEPS = 180\n\n\nclass Tick(artist.Artist):\n    \"\"\"\n    Abstract base class for the axis ticks, grid lines and labels\n\n    1 refers to the bottom of the plot for xticks and the left for yticks\n    2 refers to the top of the plot for xticks and the right for yticks\n\n    Publicly accessible attributes:\n\n      :attr:`tick1line`\n\n>>>> license: mit\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Preprocessing\n\n**In the data preprocessing phase, we need to perform two essential steps to prepare the text data for the Causal Language Model: tokenization and data collation.**\n\n#### Tokenization:\nIt involves breaking down the raw text (Python code in this case) into smaller units called tokens. These tokens could be individual words, subwords, or characters, depending on the specific requirements of the model. Once tokenized, the next step is to convert these tokens into numerical representations. This conversion enables the model to understand and process the data effectively, as neural networks work with numerical inputs.\n\n#### Data Collation:\nAfter tokenization, the data needs to be organized into sequences that the Causal Language Model can use for training. Since models typically learn from fixed-length sequences, we create data samples by sliding a window over the tokenized text. This means we extract consecutive token sequences from the code, making each sequence a training instance. By using this approach, the model can learn patterns and dependencies within the code, which is crucial for accurate Python code generation.\n\n**Together, these two preprocessing steps pave the way for successful training of the Causal Language Model, enabling it to effectively generate Python code based on the patterns it learns from the prepared tokenized and collated data.**","metadata":{}},{"cell_type":"code","source":"#Instantiate the tokenizer\nfrom transformers import AutoTokenizer\n\ncontext_length = 128\ntokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:43:10.611861Z","iopub.execute_input":"2023-07-27T09:43:10.612293Z","iopub.status.idle":"2023-07-27T09:43:14.044128Z","shell.execute_reply.started":"2023-07-27T09:43:10.612259Z","shell.execute_reply":"2023-07-27T09:43:14.043095Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/265 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ac3b94a1f654d38bb58525c6968c0a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/789k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1992d94f2d0b471f8d178f63b2373a45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/448k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7481d376469a45c2befba8aea4642e1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed092482b7754b479f004db86416c715"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac3a57c52f4f40c2b8c545970e9d39ab"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Before tokenizing the whole dataset, it is a good practice to check it on the sample inputs.**","metadata":{}},{"cell_type":"code","source":"#Let's tokenzie first 2 samples\nsamp_token = tokenizer(\n    dataset[\"train\"][:2][\"content\"],\n    truncation=True,\n    max_length=context_length,\n    return_overflowing_tokens=True,\n    return_length=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:43:14.045737Z","iopub.execute_input":"2023-07-27T09:43:14.046441Z","iopub.status.idle":"2023-07-27T09:43:14.131036Z","shell.execute_reply.started":"2023-07-27T09:43:14.046404Z","shell.execute_reply":"2023-07-27T09:43:14.130048Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Check the fields in the tokenized object\nsamp_token.keys()","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:43:14.133082Z","iopub.execute_input":"2023-07-27T09:43:14.133450Z","iopub.status.idle":"2023-07-27T09:43:14.141447Z","shell.execute_reply.started":"2023-07-27T09:43:14.133414Z","shell.execute_reply":"2023-07-27T09:43:14.140374Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"dict_keys(['input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'])"},"metadata":{}}]},{"cell_type":"code","source":"print(f\"Input IDs length: {len(samp_token['input_ids'])}\")\nprint(f\"Input chunk lengths: {(samp_token['length'])}\")\nprint(f\"Chunk mapping: {samp_token['overflow_to_sample_mapping']}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:43:14.146748Z","iopub.execute_input":"2023-07-27T09:43:14.147103Z","iopub.status.idle":"2023-07-27T09:43:14.153881Z","shell.execute_reply.started":"2023-07-27T09:43:14.147077Z","shell.execute_reply":"2023-07-27T09:43:14.152751Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Input IDs length: 167\nInput chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 21, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 105]\nChunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Since the tokenizer works fine on the sample data, we can apply it on the whole dataset using Dataset.map() method**","metadata":{}},{"cell_type":"code","source":"#Define a function to tokenzie the whole dataset\ndef tokenize_ftn(element):\n    outputs = tokenizer(\n        element[\"content\"],\n        truncation=True,\n        max_length=context_length,\n        return_overflowing_tokens=True,\n        return_length=True,\n    )\n    input_batch = []\n    \n    #Discard the tokens that are smaller than the context size\n    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n        if length == context_length:\n            input_batch.append(input_ids)\n    return {\"input_ids\": input_batch}","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:43:14.155363Z","iopub.execute_input":"2023-07-27T09:43:14.155865Z","iopub.status.idle":"2023-07-27T09:43:14.164143Z","shell.execute_reply.started":"2023-07-27T09:43:14.155832Z","shell.execute_reply":"2023-07-27T09:43:14.163122Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Check the tokenization function on some sample input\nfor sample in dataset[\"train\"]:\n    tokenized_sample = tokenize_ftn(sample)\n    break\ntokenized_sample.keys()","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:43:14.165534Z","iopub.execute_input":"2023-07-27T09:43:14.166415Z","iopub.status.idle":"2023-07-27T09:43:14.241962Z","shell.execute_reply.started":"2023-07-27T09:43:14.166381Z","shell.execute_reply":"2023-07-27T09:43:14.240822Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"dict_keys(['input_ids'])"},"metadata":{}}]},{"cell_type":"code","source":"#Apply the tokenization function on the whole dataset\ntokenized_dataset = dataset.map(function=tokenize_ftn,\n                                batched=True,\n                                remove_columns=dataset[\"train\"].column_names)\ntokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:43:14.243714Z","iopub.execute_input":"2023-07-27T09:43:14.244144Z","iopub.status.idle":"2023-07-27T09:44:19.465235Z","shell.execute_reply.started":"2023-07-27T09:43:14.244109Z","shell.execute_reply":"2023-07-27T09:44:19.464128Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d577e075fe84f2c88b45e13eb55c800"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6094732003b4a4aba139b6672d5963c"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids'],\n        num_rows: 139434\n    })\n    valid: Dataset({\n        features: ['input_ids'],\n        num_rows: 14567\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model\n\n**In our pursuit of generating Python code with utmost proficiency, we will harness the capabilities of the GPT-2 language model. Having undergone extensive training on an extensive corpus of text, GPT-2 has developed a remarkable aptitude for comprehending and predicting patterns within the provided data. To tailor the model to our specific needs, we can fine-tune a pre-trained GPT-2 model.**\n\n**Nonetheless, given the substantial amount of data available, a prudent approach would be to train the model from scratch. This allows us to customize the model's understanding of Python code and adapt it precisely to the intricacies of our dataset. Training from scratch ensures that the model becomes intimately familiar with the specific nuances and complexities of Python syntax, thereby optimizing its code generation capabilities.**","metadata":{}},{"cell_type":"code","source":"#Load the configuration file for the GPT-2 model\nfrom transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig\n\n#Load the configuration file for the GPT-2 model\nconfig = AutoConfig.from_pretrained(pretrained_model_name_or_path=\"gpt2\",\n                                    vocab_size=len(tokenizer),\n                                    n_ctx=context_length,\n                                    bos_token_id=tokenizer.bos_token_id,\n                                    eos_token_id=tokenizer.eos_token_id,\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:44:19.466495Z","iopub.execute_input":"2023-07-27T09:44:19.466842Z","iopub.status.idle":"2023-07-27T09:44:30.539526Z","shell.execute_reply.started":"2023-07-27T09:44:19.466809Z","shell.execute_reply":"2023-07-27T09:44:30.538461Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1f5d88860f8470e9ada16cd8b7d6aa3"}},"metadata":{}}]},{"cell_type":"code","source":"#Print the configuration file\nconfig","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:44:30.541172Z","iopub.execute_input":"2023-07-27T09:44:30.542621Z","iopub.status.idle":"2023-07-27T09:44:30.550126Z","shell.execute_reply.started":"2023-07-27T09:44:30.542585Z","shell.execute_reply":"2023-07-27T09:44:30.549250Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"GPT2Config {\n  \"_name_or_path\": \"gpt2\",\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 0,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 0,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 128,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"transformers_version\": \"4.30.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 50000\n}"},"metadata":{}}]},{"cell_type":"code","source":"#Instantiate the model with the configuration file\nmodel = TFGPT2LMHeadModel(config=config)\nmodel(model.dummy_inputs)  # Builds the model\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:44:30.551449Z","iopub.execute_input":"2023-07-27T09:44:30.553098Z","iopub.status.idle":"2023-07-27T09:44:38.445206Z","shell.execute_reply.started":"2023-07-27T09:44:30.553062Z","shell.execute_reply":"2023-07-27T09:44:38.444183Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Model: \"tfgpt2lm_head_model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n transformer (TFGPT2MainLaye  multiple                 124242432 \n r)                                                              \n                                                                 \n=================================================================\nTotal params: 124,242,432\nTrainable params: 124,242,432\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"#Instantiate the data collator\nfrom transformers import DataCollatorForLanguageModeling\n\ntokenizer.pad_token = tokenizer.eos_token\ndata_collator = DataCollatorForLanguageModeling(tokenizer,\n                                                mlm=False,\n                                                return_tensors=\"tf\")","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:44:38.446821Z","iopub.execute_input":"2023-07-27T09:44:38.447530Z","iopub.status.idle":"2023-07-27T09:44:38.453237Z","shell.execute_reply.started":"2023-07-27T09:44:38.447468Z","shell.execute_reply":"2023-07-27T09:44:38.452167Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#Check the data collator on some samples\ncollated_sample = data_collator([tokenized_dataset[\"train\"][i] for i in range(5)])\nfor key in collated_sample:\n    print(f\"{key} shape: {collated_sample[key].shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:44:38.454829Z","iopub.execute_input":"2023-07-27T09:44:38.455500Z","iopub.status.idle":"2023-07-27T09:44:38.478702Z","shell.execute_reply.started":"2023-07-27T09:44:38.455467Z","shell.execute_reply":"2023-07-27T09:44:38.477832Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"input_ids shape: (5, 128)\nattention_mask shape: (5, 128)\nlabels shape: (5, 128)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Create tf.data.Dataset object\ntf_train_dataset = model.prepare_tf_dataset(tokenized_dataset[\"train\"],\n                                            collate_fn=data_collator,\n                                            shuffle=True,\n                                            batch_size=32)\n\ntf_eval_dataset = model.prepare_tf_dataset(tokenized_dataset[\"valid\"],\n                                           collate_fn=data_collator,\n                                           shuffle=False,\n                                           batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:44:38.480069Z","iopub.execute_input":"2023-07-27T09:44:38.480455Z","iopub.status.idle":"2023-07-27T09:44:38.873701Z","shell.execute_reply.started":"2023-07-27T09:44:38.480424Z","shell.execute_reply":"2023-07-27T09:44:38.872765Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#Log in to the HuggingFace account\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:44:38.875222Z","iopub.execute_input":"2023-07-27T09:44:38.875553Z","iopub.status.idle":"2023-07-27T09:44:38.905128Z","shell.execute_reply.started":"2023-07-27T09:44:38.875520Z","shell.execute_reply":"2023-07-27T09:44:38.904064Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd33be39647e4c22b51d96c347e8409e"}},"metadata":{}}]},{"cell_type":"code","source":"#Define the optimizer\nfrom transformers import create_optimizer\nimport tensorflow as tf\n\nnum_train_epochs = 5\nnum_train_steps = len(tf_train_dataset) * num_train_epochs\noptimizer, schedule = create_optimizer(init_lr=5e-5,\n                                       num_warmup_steps=1_000,\n                                       num_train_steps=num_train_steps,\n                                       weight_decay_rate=0.01)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:44:38.906633Z","iopub.execute_input":"2023-07-27T09:44:38.906993Z","iopub.status.idle":"2023-07-27T09:44:38.919185Z","shell.execute_reply.started":"2023-07-27T09:44:38.906959Z","shell.execute_reply":"2023-07-27T09:44:38.917996Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#Complie the model\nmodel.compile(optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T09:44:38.920536Z","iopub.execute_input":"2023-07-27T09:44:38.921264Z","iopub.status.idle":"2023-07-27T09:44:38.943358Z","shell.execute_reply.started":"2023-07-27T09:44:38.921230Z","shell.execute_reply":"2023-07-27T09:44:38.942453Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#Define the callbacks\nfrom transformers.keras_callbacks import PushToHubCallback\n\ncallback = PushToHubCallback(output_dir=\"python-code-generator\",\n                             tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T10:23:04.512982Z","iopub.execute_input":"2023-07-27T10:23:04.513784Z","iopub.status.idle":"2023-07-27T10:23:07.157574Z","shell.execute_reply.started":"2023-07-27T10:23:04.513745Z","shell.execute_reply":"2023-07-27T10:23:07.156593Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Cloning https://huggingface.co/MUmairAB/python-code-generator into local empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"#train the model\nmodel.fit(tf_train_dataset,\n          validation_data=tf_eval_dataset,\n          callbacks=[callback],\n          epochs=5)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T10:23:11.191904Z","iopub.execute_input":"2023-07-27T10:23:11.192300Z","iopub.status.idle":"2023-07-27T14:28:56.088970Z","shell.execute_reply.started":"2023-07-27T10:23:11.192268Z","shell.execute_reply":"2023-07-27T14:28:56.088016Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Epoch 1/5\n4357/4357 [==============================] - 2917s 664ms/step - loss: 4.8998 - val_loss: 3.6062\nEpoch 2/5\n4357/4357 [==============================] - 2948s 677ms/step - loss: 3.3262 - val_loss: 3.0415\nEpoch 3/5\n4357/4357 [==============================] - 2944s 676ms/step - loss: 2.8622 - val_loss: 2.7799\nEpoch 4/5\n4357/4357 [==============================] - 2949s 677ms/step - loss: 2.6127 - val_loss: 2.6525\nEpoch 5/5\n4357/4357 [==============================] - 2910s 668ms/step - loss: 2.4821 - val_loss: 2.6055\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f32364fb010>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}